# HRAI Mind v3 — Master Rebuild Prompt

**Role:**  
System / Lead Architect for offline-first WebGPU AI Chat App.

**Objective:**  
Rebuild HRAI Mind from scratch into **Version 3** with:
- WebLLM inference in Web Worker  
- Multi-session chat with Dexie DB  
- Infinite-scroll + Virtualized rendering  
- Stop button partial-response save  
- Auto-scroll stability  
- Sanitized Markdown output  
- Zod-based settings validation  
- True offline PWA model caching (Workbox + Range requests)

---

**Context:**  
Earlier versions had issues:
- Model inference ran on **main thread** → UI freeze  
- History was always sent as **empty array** → AI memory loss  
- Stop button **discarded** partial tokens  
- Full chat history loaded at once → big chats froze browser  
- Message list re-rendered on every keystroke  
- PWA cached shell only → model not cached offline  
- Raw markdown rendered → possible **XSS**  
- STT/TTS crashed on unsupported browsers  

Now we fix all of these in V3.

---

## **Instructions**

### **Instruction 1: AI Execution Must Run in Worker**
- Implement **`ai.worker.ts`** to load model + run inference.
- UI must communicate using **singleton `worker-client.ts`**.
- Streaming tokens must be emitted incrementally.
- **Stop button** triggers `AbortController` → worker → `engine.interrupt()`.

### **Instruction 2: Multi-Session Chat with Dexie**
- Use Dexie DB with **3 tables**:
  - `chatSessions`
  - `chatMessages`
  - `metrics`
- Each new chat auto-creates a session entry.
- Session title = first user message (e.g., 1st 30 chars).

### **Instruction 3: Message Rendering & Performance**
- Use **`VariableSizeList`** (not FixedSizeList).
- Measure row height dynamically using `ResizeObserver`.
- Implement **pagination + infinite scroll**:
  - Load latest page first.
  - Older messages load when scrolling up / "Load More".
- Wrap `MessageList` in **`React.memo`** to avoid unnecessary rerenders.

---

## **Additional Requirements**

### **Markdown Rendering Rules**
- The assistant must **always respond in GitHub-Flavored Markdown**.
- Allowed formatting:
  - Headings (`#`, `##`)
  - Bullet lists
  - Code fences:  
    ```  
    ```js  
    your code  
    ```
    ```
- **Not allowed**:
  - `<script>`, `<img>`, `<iframe>`, inline styles, `onerror` attributes.
- Before rendering:  
  `DOMPurify.sanitize(marked(output))`

### **Prompt Memory (Context Windowing)**
- Never send whole chat history.
- Send:  
  **system + last N messages** (token budget controlled).

### **Auto-Scroll UX**
- After send → scroll to bottom.  
- During streaming → keep at bottom.  
- After loading history page 1 → scroll to bottom once.

### **Stop Button Partial Save**

if (error === "aborted" && streamedRef.current.trim()) {
save streamedRef.current to Dexie as assistant reply
}


### **STT/TTS Feature Detection**
- If unsupported → disable UI button + show toast.
- Never crash UI.

### **Zod Settings Guard**
- Parse localStorage through **`safeParse`**.
- On fail → restore **DEFAULT_SETTINGS**.

### **PWA Offline Model Support**
- Use Workbox runtime caching.
- Model binaries cached with **HTTP Range support**.
- After one successful download → app works **fully offline**.

### **Type Safety**
- Enable:
  - `"strict": true`
  - `"noUncheckedIndexedAccess": true`
- Fix type errors during rebuild.

### **React Stability**
- Use **Error Boundary** to avoid white screens.

---

## **Notes**
**Note 1:**  
UI must **never** freeze during model loading or inference.

**Note 2:**  
Do **not** store full message history in React memory; fetch + append only as needed.

**Note 3:**  
Virtualization + pagination **must both be active** to support 5,000+ message chats.

